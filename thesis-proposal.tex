\documentclass{article}

% \usepackage{geometry}
% \geometry{verbose,a4paper,tmargin=20mm,bmargin=25mm,lmargin=30mm,rmargin=30mm}
% \newcommand{\dataset}{{\cal D}}
% \newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
% \setlength{\parindent}{0pt}

\usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\pagenumbering{gobble}

\begin{document}

\title{\Large Thesis proposal \\ \small An alpha version of the draft}
\author{Nil Adell Mill \\
        Winter 2020 \\
        Institute of Neuroinformatics \\}
\date{Spring 2019}

\maketitle

    \section*{Introduction/Background}  

Drug discovery—the process by which a potential new medicine is identified---is a
 complex process that encompases the intersection of several fields (such as biology,
 statistics, chemistry or pharmacology). The entire process is a long and costly
 endeavour, with a typical time-frame of 10 to 20 years till maket release and an
 estimated cost between 1 and 2 billion USD. With just a small quantity of the initially
 identified compounds actually becoming an approved medicine. Many of these dropouts
 happening at the early stages of the entire pipeline.

It exists, then, a need for better mechanisms for detecting better candidates. One of
 the most promising directions is to improve the \emph{in-silico}
 methods---computational simulations are relatively cheap and quick run that makes them
 an interesting solution. \emph{In-silico} simulations then cover two main aspects:
 modelling the dynamics of the human body---such that any effect relevant to the durg or
 the disease will be captured by it---and methods to generate good candidates that are
 effective at exploring the vast space of possible compounds.


% To do so, two main components are required: a model accurate enough of the human body,
% such that it reflects reality properly; a way to generate good candidates that is
% effective at exploring the vast space of possible compounds.

% Those two components may be subdivided into smaller parts and abstracted in order to
% account for different elements, for example, one may only look at ic50 as a metric for
% drug response or one may try to generate new compounds by exploring compounds
% chemically similar to already in use drugs. At the same time, many different methods
% may be used to tackle all these questions.


Among the different computational approaches that have been used in the process of drug
 discovery deep learning (DL) has shown signs to be a potential game changer
 \cite{Dargan2019}. DL has been able to capitalize on the exponential growth of data and
 the higher availability of computational resources. For examle, DL has had a remarkable
 success on computer vision (CV) and natural language processing (NLP), and has become
 the go-to solution for any problem in these two fields. It is, at the same time,
 penetrating into other fields, drug-discovery being one of them \cite{Chen2018}. 

When we deal with this biological and molecular data, it exists a challenge on how to
 deal with the intrinsic structure of the data.
%  When it comes to the methodology it exists an orthogonal problem [regarding
%  strcutured data] when we deal with biological data.
 If we look at the case of deep learning for CV, where we deal with images, a key
 element of any architecture for it's success was the use of convolutional layers---one
 will mostly observe convolutional neural networks (CNNs) when analizying the state of
 the art in CV---which introduce a structural a prior based on the structure of the
 data. A similar case can be made for NLP. For that reason, there exists a strong signal
 to look for models that can leverage the structural equivalent when in molecule or
 protein data, i.e. leverage graph structures \cite{Wu2019}. Sign [Sign? need to rewrite that] of that
 is the recent advancements in that direction \cite{Sun2019}.
 
%  [There is already a literature on this and I'm gonna talk about it \cite{Sun2019}]

Another of the big challenges is to unify all the aspects of drug-discovery and be able
 to incorporate all the rellevant biological information when designing possible
 candidate molecules. An initial success story on that line is a recently paper
 \cite{Zhavoronkov2019} where the authors describe a deep learning method by which they
 are able to discover inhibitors of discoidin domain receptor 1 (DDR1)—a kinase
 implicated in fibrosis—in just 21 days.

Those promising results, albeit encouraging, are just the tip of the iceberg. There is
still a long way till a model can satisfactorily capture the biological complexity of
any arbitrary target and produce promising candidates. On top of that, there is an added
dimension, as such model should account for the variability from patient to patient and
be able to generate a molecule that accomodates for all the genotipic and phenotipic
variants, or generate different candidates for each of the genetic populations of
interest. [need a ref here]

[I am not completely sure about this paragraph but I leave it here so I don't forget for
now] Even more, in the case of diseases like cancer, an heterogeneous population may
appear within a single patient. So the same variant effects arise inside a dynamic
ecosystem, where a drug that just targets a subpopulation may lead to an evolutionary
pressure complicating further the treatment outlook [reference paper of evolutionary
perspective to cancer].

There is then a great need to develop models that can be conditioned based on a large
set of biological [conditions?] and meaningfully account for this variations when
generating a compound or/and evaluating a compunds effect when administered.

In fact it is of interest to develop multi-scale models that capture system complexity
at the different levels. For instance, a model that is able to learn protein-compound
interactions---commonly known as the docking problem---while at the same time use this
information to predict effects of the introduction of the compound on the larger
protein-protein interaction (PPI) network.

    \section*{Aim \& Methods}
[Should I separate em in two different sections?]

The aim of this thesis will be two fold. One the one side, analyze how the explicit use
of graph convolutional neural networks (GCNNs) may open new oportunities when dealing
with biological and checmical data. On the other side, explore how modelling the biology
at different levels (e.g. molecular structure v.s. molecular interacton network [okay
here I need to develop furhter about PPI, maybe mention NetBite (as Jannis referenced in
the mail)]) may help with our understanding [of the biology? of compounds interaction?]
and help generate better models. Furthermore, evaluate how these may be integrated
toguether.

This precise work will be focused around exploring all these concepts in the context of
drug design for cancer [...] the work will be done in colaboration with the
Computational Systems Biology group at IBM Research (Zurich). [...] The group is
currently focused on individualised paediatric cure (iPC), so an end goal of this
project is for the end results of it to help in that effor, for instance in contibuting
to the ongoing research in neuroblastoma.

As mentioned previously, the idea of using GCNNs is not a new one in the literature
\cite{Sun2019}. My project will build upon those ideas presented in the literature,
expand them and test their feasibility by implementing them into a wider framework for
drug design \cite{Born2019}. In that context two main areas of application appear. One
of them would be to re-desing the drug conditional generator, for instance by reframing
the vairational autoencoders, used for molecule generation, to architectures that
operate over graphs \cite{Simonovsky2018,Li2018,Li2018a}. The second area would be to
find better ways to asses the activity of these molecules, and in a wider context,
assess their relevance as drug candidates. In the concrete case of the mentioned
framework it is done by using a critic network proposed in \cite{Manica2019}. This could
be expanded on a set of different fronts: usign structural data instead of SMILES
\cite{Li,Do2019}, by using GCNNs to cover a much wider network of genes
\cite{Oskooei2019, Wang2019}, or by introducing particular scores (rewards) based in the
interaction of the compound to certain targets \cite{YingkaiGao2018, Zhavoronkov2019} or
the combination of the compund with other drugs \cite{Zitnik2018} ---a common practice
in patients with cancer.

All these possible changes on the critic model would apply at different abstraction
levels. That opens the door to seek for ways to integrate the representations learnt at
those different stages \cite{Ying2018, Ma2019, Huang2019}. On top of that information
extracted from here could be then leveraged on the drug generation part of the
framework.

% Among the pleathora of techniques to be explored in order to improve models we will be
%  focusing on how information from graph structures can be extracted and used for these
%  pourposes. In partucular stuidying protein protein interaction (PPI) networks may
%  unravel relationships in the cell that are being not currently taken into account when
%  analyzing a perturbation in the system—for instance introducing a drug or knocking out
%  a particular protein. Some papers [paper jannis mentioned in the email] have already
%  indicated that that may be the casee...\dots

% Another case to look at would be to model the proteins itself in a  way that
%  represents them in a more complete way?[more natural way], explicitly accounting for
%  their structure and how different parts of it may interact. A successful example of
%  that: Deep learning enables rapid identification of potent DDR1 kinase inhibitors has
%  recently been published in the literature and opens the door for further improvements
%  on the biological modelling....




% \section*{Methods}
% \subsection*{Differential Privacy}
% There have been several proposals in the literature on how to properly implement DP.
% I will particularly focus on {\em Private Aggregation of Teacher Ensembles} (PATE)
% \cite{pate} and it's generative variant, PATE-G. In PATE, the data is used to train
% a set of teacher models, the aggregated output of which will be used as a method to
% train a student model—our actual prediction model. So, we end up operating in a
% framework where the data and the model are not directly connected. My task will be
% to study the robustness and utility of this system outside toy setups. Specifically,
% I will see how it can be used over genetic data and replicate literature results
% where this methodology was not used—such that we can evaluate the performance of
% this approach against the reported results in the literature. I plan to constrain
% the literature to a particular topic. As a starting point, I will investigate the
% integration of this framework to the topic of genomics and neurodevelopmental
% disorders \cite{genomics-neuro}, more specifically, my focus is going to be on
% genome-wide association methods \cite{genomics-autism, autism-dl, zhou2, ml-autism}.
% Nevertheless, the final details of the subtopic may change depending on a set of
% factors (i.e. replicability, dataset and model availability, computational power
% requirements, etc.) to be evaluated during the development of the project.

% {\color{grayishgray}[I don't really want to marry and promise too much on a topic as what is important is the methods that I apply to it, not the topics "per se". However as how I wrote it sounds super vague and "weak"... idk at the end of the day literature review is part of the project...]}

% \subsection*{Federated Learning}
% On the federated learning space, I will develop and evaluate a feasible prototype of a distributed system. I plan to build upon the results and solutions of the previous section. The initial approach will be to study and evaluate already proposed federated learning strategies \cite{fed-learning, fed-learning2} for generic types of data. Following that, I will develop on that base to generate a working framework for the specific models and data mentioned previously. 

% This work will be first carried out in a simulated manner inside a virtual
% environment, with the aim of physical implementation—although that will likely
% fall outside the scope of this project due to time and resources constraints.
% Ultimately this project should serve as a proof of concept for a solution
% applicable in healthcare. 



% {\color{red}\#\# NOTE: SHOULD I ADD THIS?} 
% \subsection*{Practicalities}
% The described work will be carried out at
% \emph{Decentriq}\footnote{https://decentriq.ch/} under the supervision of Stefan
% Deml. \emph{Decentriq} focuses its operations on combining machine learning and data
% analytics with cryptography and privacy preserving techniques to allow
% confidentiality when operating with private data.  Stefan is one of the co-founders
% of \emph{Decentriq}, he is an ETH Zürich graduate, he has previously co-founded an
% ETH Spin-off and worked for the Ethereum foundation.

% All methods, software, and results will be open sourced.

% \subsection*{Something else?}

% \section*{Software and Hardware Requirements}
% Outline what your specific requirements will be with regard
% to software and hardware, but note that any special requests
% might need to be approved by your supervisor and the Head of
% Department.

% Overall, you should aim to produce roughly a two-page document
% (and certainly no more than four pages)
% outlining your plan for the year.
\bibliographystyle{apalike}
\bibliography{thesis-proposal.bib}
    
% \end{multicols}
\end{document}    
