\documentclass{article}

% \usepackage{geometry}
% \geometry{verbose,a4paper,tmargin=20mm,bmargin=25mm,lmargin=30mm,rmargin=30mm}
% \newcommand{\dataset}{{\cal D}}
% \newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
% \setlength{\parindent}{0pt}

\usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\pagenumbering{gobble}

\begin{document}

\title{\Large Thesis proposal \\ \small An alpha version of the draft}
\author{Nil Adell Mill \\
        Winter 2020 \\
        Institute of Neuroinformatics \\}
\date{Spring 2019}

\maketitle

    \section*{Introduction/Background}  

Drug discovery—the process by which a potential new medicine is identified---is a
 complex process that encompases the intersection of several fields (such as biology,
 statistics, chemistry or pharmacology). The entire process is a long and costly
 endeavour, with a typical time-frame of 10 to 20 years till maket release and an
 estimated cost between 2 and 3 billion USD \cite{Schneider2019, Scannell2012}. With
 just a small quantity of the initially identified compounds actually becoming an
 approved medicine. Many of these dropouts happening at the early stages of the entire
 pipeline.

It exists, then, a need for better mechanisms for detecting better candidates. One of
 the most promising directions is to improve the \emph{in-silico}
 methods---computational simulations are relatively cheap and quick run that makes them
 an interesting solution. \emph{In-silico} simulations then cover two main aspects:
 \textbf{predictive modelling}, meaning modelling the dynamics of the human body---such
 that any effect relevant to the durg or the disease will be captured by it---and
 \textbf{generative modelling}, i.e. methods to generate good candidates that are
 effective at exploring the vast space of possible compounds, estimated to be on the
 order of $10^{60}$\cite{Reymond2012}.


% To do so, two main components are required: a model accurate enough of the human body,
% such that it reflects reality properly; a way to generate good candidates that is
% effective at exploring the vast space of possible compounds.

% Those two components may be subdivided into smaller parts and abstracted in order to
% account for different elements, for example, one may only look at ic50 as a metric for
% drug response or one may try to generate new compounds by exploring compounds
% chemically similar to already in use drugs. At the same time, many different methods
% may be used to tackle all these questions.


Among the different computational approaches that have been used in the process of drug
 discovery deep learning (DL) has shown signs to be a potential game changer
 \cite{Dargan2019}. DL has been able to capitalize on the exponential growth of data and
 the higher availability of computational resources. For examle, DL has had a remarkable
 success on computer vision (CV) and natural language processing (NLP), and has become
 the go-to solution for any problem in these two fields. It is, at the same time,
 penetrating into other fields, drug-discovery being one of them \cite{Chen2018}. 

When we deal with this biological and molecular data, it exists a challenge on how to
 deal with the intrinsic structure of the data.
%  When it comes to the methodology it exists an orthogonal problem [regarding
%  strcutured data] when we deal with biological data.
 If we look at the case of deep learning for CV, where we deal with images, a key
 element of any architecture for it's success was the use of convolutional layers---one
 will mostly observe convolutional neural networks (CNNs) when analizying the state of
 the art in CV---which introduce a structural a prior based on the structure of the
 data. A similar case can be made for NLP. For that reason, there exists a strong signal
 to look for models that can leverage the structural equivalent when in molecule or
 protein data, i.e. leverage graph structures \cite{Wu2019}. In fact, there have been
 several models as such being proposed in the literature \cite{Sun2019}.
 %Sign of that is the recent advancements in that direction \cite{Sun2019}.
 % [There is already a literature on this and I'm gonna talk about it \cite{Sun2019}]

Another of the big challenges is to unify all the aspects of drug-discovery and be able
 to incorporate all the rellevant biological information when designing possible
 candidate molecules. An initial success story on that line is a recently paper
 \cite{Zhavoronkov2019} where the authors describe a deep learning method by which they
 are able to discover inhibitors of discoidin domain receptor 1 (DDR1)—a kinase
 implicated in fibrosis—in just 21 days.

Those promising results, albeit encouraging, are just the tip of the iceberg. There is
still a long way till a model can satisfactorily capture the biological complexity of
any arbitrary target and produce promising candidates. On top of that, there is an added
dimension, as such model should account for the variability from patient to patient and
be able to generate a molecule that accomodates for all the genotipic and phenotipic
variants, or generate different candidates for each of the genetic populations of
interest. [need a ref here]

[I am not completely sure about this paragraph but I leave it here so I don't forget for
now] Even more, in the case of diseases like cancer, an heterogeneous population may
appear within a single patient. So the same variant effects arise inside a dynamic
ecosystem, where a drug that just targets a subpopulation may lead to an evolutionary
pressure complicating further the treatment outlook [reference paper of evolutionary
perspective to cancer].

There is then a great need to develop models that can be conditioned based on a large
set of biological [conditions?] and meaningfully account for this variations when
generating a compound or/and evaluating a compunds effect when administered.

In fact it is of interest to develop multi-scale models that capture system complexity
at the different levels. For instance, a model that is able to learn protein-compound
interactions---commonly known as the docking problem---while at the same time use this
information to predict effects of the introduction of the compound on the larger
protein-protein interaction (PPI) network.

    \section*{Aim \& Methods}
[Should I separate em in two different sections?]

The aim of this thesis will be two fold. One the one side, analyze how the explicit use
of graph convolutional neural networks (GCNNs) may open new oportunities when dealing
with biological and checmical data. On the other side, explore how modelling the biology
at different levels (e.g. molecular structure v.s. molecular interacton network [okay
here I need to develop furhter about PPI, maybe mention NetBite (as Jannis referenced in
the mail)]) may help with our understanding [of the biology? of compounds interaction?]
and help generate better models. Furthermore, evaluate how these may be integrated
toguether.

This precise work will be focused around exploring all these concepts in the context of
drug design for cancer [...] the work will be done in colaboration with the
Computational Systems Biology group at IBM Research (Zurich). [...] The group is
currently focused on individualised paediatric cure (iPC), so an end goal of this
project is for the end results of it to help in that effor, for instance in contibuting
to the ongoing research in neuroblastoma.

As mentioned previously, the idea of using GCNNs is not a new one in the literature
\cite{Sun2019}. My project will build upon those ideas presented in the literature,
expand them and test their feasibility by implementing them into a wider framework for
drug design \cite{Born2019}. In that context two main areas of application appear. One
of them would be to re-desing the drug conditional generator, for instance by reframing
the vairational autoencoders, used for molecule generation, to architectures that
operate over graphs \cite{Simonovsky2018,Li2018,Li2018a}. The second area would be to
find better ways to asses the activity of these molecules, and in a wider context,
assess their relevance as drug candidates. In the concrete case of the mentioned
framework it is done by using a critic network proposed in \cite{Manica2019}. This could
be expanded on a set of different fronts: usign structural data instead of SMILES
\cite{Li,Do2019}, by using GCNNs to cover a much wider network of genes
\cite{Oskooei2019, Wang2019}, or by introducing particular scores (rewards) based in the
interaction of the compound to certain targets \cite{YingkaiGao2018, Zhavoronkov2019} or
the combination of the compund with other drugs \cite{Zitnik2018} ---a common practice
in patients with cancer.

All these possible changes on the critic model would apply at different abstraction
levels. That opens the door to seek for ways to integrate the representations learnt at
those different stages \cite{Ying2018, Ma2019, Huang2019}. On top of that information
extracted from here could be then leveraged on the drug generation part of the
framework.

% \subsection*{Practicalities}

% Work at IBM?

% \subsection*{Something else?}

% \section*{Software and Hardware Requirements}
% Outline what your specific requirements will be with regard
% to software and hardware, but note that any special requests
% might need to be approved by your supervisor and the Head of
% Department.

% Overall, you should aim to produce roughly a two-page document
% (and certainly no more than four pages)
% outlining your plan for the year.


\bibliographystyle{apalike}
\bibliography{thesis-proposal.bib}
    
\end{document}    
