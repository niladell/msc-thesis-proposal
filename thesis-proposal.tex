\documentclass{article}

% \usepackage{geometry}
% \geometry{verbose,a4paper,tmargin=20mm,bmargin=25mm,lmargin=30mm,rmargin=30mm}
% \newcommand{\dataset}{{\cal D}}
% \newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
% \setlength{\parindent}{0pt}

\usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\pagenumbering{gobble}

\begin{document}

\title{\Large Thesis proposal}
\author{Nil Adell Mill \\
        Winter 2020 \\
        Institute of Neuroinformatics \\}
\date{Spring 2019}

\maketitle

    \section*{Introduction/Background}  

Drug discovery---the process by which a potential new medicine is identified---is a
 complex process that encompasses the intersection of several fields (such as biology,
 statistics, chemistry or pharmacology). The entire process is a long and costly
 endeavor, with a typical time-frame of 10 to 20 years till market release and an
 estimated cost between 2 and 3 billion USD \cite{Schneider2019, Scannell2012}. With
 just a small quantity of the initially identified compounds actually becoming an
 approved medicine—only 1 out of 10 000 synthesized molecules gets market approval one
 day. Many of these dropouts happening at the early stages of the entire pipeline.

It exists, then, a need for better mechanisms for detecting better candidates. One of
 the most promising directions is to improve the \emph{in-silico}
 methods---computational simulations are relatively cheap and quick run that makes them
 an interesting solution. \emph{In-silico} simulations then cover two main aspects:
 \textbf{predictive modelling}, meaning modeling the dynamics of the human body---such
 that any effect relevant to the drug or the disease will be captured by it---and
 \textbf{generative modelling}, i.e. methods to generate good candidates which, in part,
 means methods that are effective at exploring the vast space of possible compounds,
 estimated to be on the order of $10^{60}$\cite{Reymond2012}. Several computational
 approaches have been used over the years, from modeling molecular dynamics simulations
 to data-driven statistical methods \cite{Hung2014, Kuhn2016}. Recently deep learning
 (DL) has shown signs to be a potential game-changer \cite{Dargan2019}. 


% To do so, two main components are required: a model accurate enough of the human body,
% such that it reflects reality properly; a way to generate good candidates that is
% effective at exploring the vast space of possible compounds.

% Those two components may be subdivided into smaller parts and abstracted in order to
% account for different elements, for example, one may only look at ic50 as a metric for
% drug response or one may try to generate new compounds by exploring compounds
% chemically similar to already in use drugs. At the same time, many different methods
% may be used to tackle all these questions.
 
DL has been able to capitalize on the exponential growth of data and the higher
 availability of computational resources. DL has had remarkable success in computer
 vision (CV) \cite{Guo2016} and natural language processing (NLP) \cite{Young2018}, and
 has become the go-to solution for any problem in these two fields. For instance, in the
 case of CV, where we deal with images, a key element of any architecture's success was
 the use of convolutional layers---one will mostly observe convolutional neural networks
 (CNNs) when analyzing the state of the art in CV---which introduces a structural a
 prior based on the structure of the data\cite{Fukushima1980, LeCun1989, Ulyanov}. A
 similar case can be made for NLP. When we deal with biological and molecular data, it
 exists a challenge and an opportunity on how to deal with this intrinsic structure,
 i.e. leveraging the knowledge that they are graphs. Efforts in generalizing the
 convolution operator on non-euclidian structures have {\color{red}[given rise to the
 appearance of]} graph convolutional neural networks (GCNNs)\cite{Wu2019}. GCNNs, at the
 same time, started penetrating the field of drug discovery \cite{Sun2019}.

%  Its architecture is a great prior to capture an inductive bias of images that fully
%  connected networks don’t : Images are a 2D structure, and pixels have neighbors, which
%  are very related to them. Thus it is very useful to compare pixels in a small zone, but
%  much less to compare two random pixels of an image.

% CNN uses convolutions, which are local operations, which act on neighbor pixels. And
% patterns on images are also local : if you want to detect a line on the image, you need
% to see if there is a continuous pattern of pixels of the same color, bordered by a
% different color.

%  It is, at the same time, penetrating into other fields, drug-discovery being one of
%  them \cite{Chen2018}. 


%  When it comes to the methodology it exists an orthogonal problem [regarding
%  strcutured data] when we deal with biological data.

%  { \color{gray} If we look at the case of deep learning for CV, where we deal with
%  images, a key element of any architecture for it's success was the use of convolutional
%  layers---one will mostly observe convolutional neural networks (CNNs) when analizying
%  the state of the art in CV---which introduce a structural a prior based on the
%  structure of the data\cite{Guo2016}. A similar case can be made for
%  NLP\cite{Young2018}. On that direction there has been a rising field on the use of
%  Graph Convolutional Neural Networks (GCNNs)\cite{Wu2019}, and in fact, there have been
%  several models as such being proposed in the drug design literature \cite{Sun2019}.
%  [NOTE: Rewrite this last section]}

 %Sign of that is the recent advancements in that direction \cite{Sun2019}.
 % [There is already a literature on this and I'm gonna talk about it \cite{Sun2019}]

Another of the big challenges is to unify all the aspects of drug-discovery and be able
 to incorporate all the relevant biological information when designing possible
 candidate molecules. An initial success story on that line is a recent paper
 \cite{Zhavoronkov2019} where the authors describe a deep learning method by which they
 are able to discover, synthesize, and test in an animal model, inhibitors of discoidin
 domain receptor 1 (DDR1)—a kinase implicated in fibrosis—in less than two months.

Those promising results, albeit encouraging, are just the tip of the iceberg. There is
still a long way until a model can satisfactorily capture the biological complexity of
an arbitrary target and produce promising candidates. On top of that, there is an added
dimension, as such model should account for the variability from patient to patient and
be able to generate a molecule that accommodates for all the genotypic and phenotypic
variants or generate different candidates for each of the genetic populations of
interest. That is especially important for hypercomplex diseases; for example in cancer
where a genotypically heterogeneous cancer population may appear within a single
patient \cite{Boland2017}. So the same variant effects arise inside a dynamic
ecosystem, where a drug that just targets a subpopulation may lead to an evolutionary
pressure complicating further the treatment outlook \cite{Enriquez-Navas2015}.

There is then a great need to develop models that can be conditioned based on a large
set of biological factors and meaningfully account for these variations when generating
a compound or/and evaluating a compound's effect when administered. What is, in other
words, the need for the wider adoption of precision medicine.

Last of all, in a more holistic view, it is of interest to develop multi-scale models
that capture system complexity at different levels. For instance, a model that can learn
protein-compound interactions---commonly known as the docking problem---while at the
same time use this information to predict effects of the introduction of the compound on
the larger protein-protein interaction (PPI) network\cite{Sun2019}.

    \section*{Aim \& Methods}

The aim of this thesis will be two-fold. One the one side, analyze how the explicit
use of GCNNs may open new opportunities when dealing with biological and chemical
data. On the other side, explore how modeling the biology at different levels (e.g.
molecular structure v.s. molecular interaction network) may help create better
models. Furthermore, evaluate how these different scales may be integrated.

This precise work will be focused on exploring all these concepts in the context of
designing anti-cancer drugs. The work will be done in collaboration with the
Computational Systems Biology group at IBM Research (Zurich), which is currently focused
on individualized pediatric cure (iPC). As such, an end goal of this project is for the
end results of it to help in that effort, for instance in contributing to the ongoing
research in neuroblastoma.

As mentioned previously, the idea of using GCNNs for drug discovery is not a new one in
the literature \cite{Sun2019}. My project will build upon those ideas presented in the
literature, expand them and test their feasibility by implementing them into a wider
framework for drug design \cite{Born2019}. In that context, two main areas of
application appear. One of them is to re-design the generative model, for instance by
reframing the variational autoencoders, used for molecule generation, to architectures
that operate over graphs \cite{Simonovsky2018,Li2018, Li2018a}. The second area is to
find better ways to asses the activity of these molecules, and in a wider context,
assess their relevance as drug candidates, i.e. improve the predictive model. In the
concrete case of the mentioned framework, it is done by using a critic network
\cite{Manica2019}. This can be expanded on a set of different fronts: using structural
data instead of SMILES\footnote{http://opensmiles.org/} \cite{Li, Do2019}, by using
GCNNs over PPI networks, like STRING\footnote{https://string-db.org/}, in a manner that
allows for the use all the information available\cite{Oskooei2019, Wang2019}, or by
introducing particular scores (rewards) based in the interaction of the compound to
certain targets \cite{YingkaiGao2018, Zhavoronkov2019} or the combination of the
compound with other drugs \cite{Zitnik2018}---a common practice in patients with cancer.
All these possible changes on the critic model would apply at different abstraction
levels. That opens the door to seek for ways to integrate the representations learnt at
those different stages \cite{Ying2018, Ma2019, Huang2019}. On top of that information
extracted from here could be then leveraged on the drug generation part of the
framework.


% \subsection*{Practicalities}

% Work at IBM?

% \subsection*{Something else?}

% \section*{Software and Hardware Requirements}
% Outline what your specific requirements will be with regard
% to software and hardware, but note that any special requests
% might need to be approved by your supervisor and the Head of
% Department.

% Overall, you should aim to produce roughly a two-page document
% (and certainly no more than four pages)
% outlining your plan for the year.


\bibliographystyle{apalike}
\bibliography{thesis-proposal.bib}
    
\end{document}    
