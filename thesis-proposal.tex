\documentclass{article}

% \usepackage{geometry}
% \geometry{verbose,a4paper,tmargin=20mm,bmargin=25mm,lmargin=30mm,rmargin=30mm}
% \newcommand{\dataset}{{\cal D}}
% \newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
% \setlength{\parindent}{0pt}

\usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\pagenumbering{gobble}

\begin{document}

\title{\Large Thesis proposal}
\author{Nil Adell Mill \\
        January 2020 \\ \\
        Institute of Neuroinformatics, \\
        ETH Zurich \& Zurich University}
\date{Spring 2019}

\maketitle

    \section*{Introduction \& Background}  

Drug discovery---the process by which a potential new medicine is identified---is a
 complex process that encompasses the intersection of several fields (such as biology,
 statistics, chemistry or pharmacology). The entire process is a long and costly
 endeavor, with a typical time-frame of 10 to 20 years till market release and an
 estimated cost between 2 and 3 billion USD \cite{Schneider2019, Scannell2012}. With
 just a small quantity of the initially identified compounds actually becoming an
 approved medicine—only 1 out of 10000 synthesized molecules gets market approval. Many
 of these dropouts happening at the early stages of the entire pipeline. On that
 context, a big challenge is to unify all the aspects of drug-discovery and be able to
 incorporate all the relevant biological information when designing possible candidate
 molecules.


%  at the same time, started penetrating the field of drug discovery \cite{Sun2019}.

%  Its architecture is a great prior to capture an inductive bias of images that fully
%  connected networks don’t : Images are a 2D structure, and pixels have neighbors, which
%  are very related to them. Thus it is very useful to compare pixels in a small zone, but
%  much less to compare two random pixels of an image.

% CNN uses convolutions, which are local operations, which act on neighbor pixels. And
% patterns on images are also local : if you want to detect a line on the image, you need
% to see if there is a continuous pattern of pixels of the same color, bordered by a
% different color.

%  It is, at the same time, penetrating into other fields, drug-discovery being one of
%  them \cite{Chen2018}. 


%  When it comes to the methodology it exists an orthogonal problem [regarding
%  strcutured data] when we deal with biological data.

%  { \color{gray} If we look at the case of deep learning for CV, where we deal with
%  images, a key element of any architecture for it's success was the use of convolutional
%  layers---one will mostly observe convolutional neural networks (CNNs) when analizying
%  the state of the art in CV---which introduce a structural a prior based on the
%  structure of the data\cite{Guo2016}. A similar case can be made for
%  NLP\cite{Young2018}. On that direction there has been a rising field on the use of
%  Graph Convolutional Neural Networks (GCNNs)\cite{Wu2019}, and in fact, there have been
%  several models as such being proposed in the drug design literature \cite{Sun2019}.
%  [NOTE: Rewrite this last section]}

 %Sign of that is the recent advancements in that direction \cite{Sun2019}.
 % [There is already a literature on this and I'm gonna talk about it \cite{Sun2019}]

Despite that, new approaches are appearing with the objective of solving those
 shortcomings. An initial success story is a recent paper \cite{Zhavoronkov2019} where
 the authors describe a method by which they are able to discover, synthesize, and test
 in an animal model, inhibitors of discoidin domain receptor 1 (DDR1)—a kinase
 implicated in fibrosis—in less than two months.


Those promising results, albeit encouraging, are just the tip of the iceberg. There is
still a long way until a model can satisfactorily capture the biological complexity of
an arbitrary target and produce promising candidates. On top of that, there is an added
dimension, as such model should account for the variability from patient to patient and
be able to generate a molecule that accommodates for all the genotypic and phenotypic
variants. Or otherwise generate different candidates for each of the populations of
interest. That is especially important for hypercomplex diseases; for example, in
cancer, where a genotypically heterogeneous tumoral population may appear within a
single patient \cite{Boland2017}. So the same variant effects arise inside a dynamic
ecosystem, where a drug that just targets a subpopulation may lead to an evolutionary
pressure complicating further the treatment outlook \cite{Enriquez-Navas2015}.

There is then a great need to develop models that can be conditioned based on a large
set of biological factors and meaningfully account for these variations when generating
a compound or/and evaluating a compound's effect when administered. What is, in other
words, the need for the wider adoption of precision medicine.

Last of all, in a more holistic view, it is of interest to develop multi-scale models
that integrate a system's complexity at all different levels. For instance, a model that
can learn protein-compound interactions---commonly known as the docking problem---while
at the same time use this information to predict effects of the introduction of the
compound on the larger protein-protein interaction (PPI) network\cite{Sun2019}.

It exists, then, a need for better mechanisms for detecting better candidates. One of
 the most promising directions is to improve the \emph{in-silico}
 methods---computational simulations are relatively cheap and quick run that makes them
 an interesting solution. \emph{In-silico} simulations then cover two main aspects:
 \textbf{predictive modelling}, meaning modeling the dynamics of the human body---such
 that any effect relevant to the drug or the disease will be captured by it---and
 \textbf{generative modelling}, i.e. methods to generate good candidates which, in part,
 means methods that are effective at exploring the vast space of possible compounds,
 estimated to be on the order of $10^{60}$\cite{Reymond2012}. Several computational
 approaches have been used over the years, from modeling molecular dynamics simulations
 to data-driven statistical methods \cite{Hung2014, Kuhn2016}. Recently deep learning
 (DL) has shown signs to be a potential game-changer \cite{Dargan2019}. 


% To do so, two main components are required: a model accurate enough of the human body,
% such that it reflects reality properly; a way to generate good candidates that is
% effective at exploring the vast space of possible compounds.

% Those two components may be subdivided into smaller parts and abstracted in order to
% account for different elements, for example, one may only look at ic50 as a metric for
% drug response or one may try to generate new compounds by exploring compounds
% chemically similar to already in use drugs. At the same time, many different methods
% may be used to tackle all these questions.
 
DL has been able to capitalize on the exponential growth of data and the higher
 availability of computational resources. DL has had remarkable success in computer
 vision (CV) \cite{Guo2016} and natural language processing (NLP) \cite{Young2018}, and
 has become the go-to solution for any problem in these two fields. For instance, in the
 case of CV, where we deal with images, a key element of any architecture's success was
 the use of convolutional layers---one will mostly observe convolutional neural networks
 (CNNs) when analyzing the state of the art in CV---which introduces a structural a
 prior based on the structure of the data\cite{Fukushima1980, LeCun1989, Ulyanov}. A
 similar case can be made for NLP. A similar approach, them, can be proposed when we
 deal with biological and molecular data by leveraging their graph representations.
 Efforts in generalizing the convolution operator on non-euclidian structures have given
 rise to graph convolutional neural networks (GCNNs)\cite{Wu2019}. GCNNs, then, pose an
 opportunity to drug discovery due to their capacity to deal natively with graph
 data\cite{Sun2019}.
 
    \section*{Aim \& Methods}

The aim of this thesis will be two-fold. One the one side, analyze how the explicit
use of GCNNs may open new opportunities when dealing with biological and chemical
data. On the other side, explore how modeling the biology at different levels (e.g.
molecular structure v.s. molecular interaction network) may help create better
models. Furthermore, evaluate how these different scales may be integrated.

This precise work will be focused on exploring all these concepts in the context of
designing anti-cancer drugs. The work will be done in collaboration with the
Computational Systems Biology group at IBM Research (Zurich), currently part of the iPC
consortium\footnote{https://ipc-project.eu/}. As part of such, an end goal of this
project is for the results of this project to help in the consortium efforts on
paediatric cancer, for instance in contributing to the ongoing research in
neuroblastoma, the most common cancer diagnosed on the first year of life
\cite{Maris2010}.

As mentioned previously, the idea of using GCNNs for drug discovery is not a new one in
the literature \cite{Sun2019}. My project will build upon those ideas presented in the
literature, expand them and test their feasibility by implementing them into a wider
framework for drug design \cite{Born2019}. In that context, two main areas of
application appear. One of them is to re-design the generative model, for instance by
reframing the variational autoencoders, used for molecule generation, to architectures
that operate over graphs \cite{Simonovsky2018,Li2018, Li2018a}. The second area is to
find better ways to asses the activity of these molecules, and in a wider context,
assess their relevance as drug candidates, i.e. improve the predictive model. In the
concrete case of the mentioned framework, it is done by using a critic network
\cite{Manica2019}. This can be expanded on a set of different fronts: using structural
data instead of SMILES\footnote{http://opensmiles.org/} \cite{Li, Do2019}, by using
GCNNs over PPI networks in a manner that allows for the use all the information
available\cite{Oskooei2019, Wang2019}, or by introducing particular scores (rewards)
based in the interaction of the compound to certain targets \cite{YingkaiGao2018,
Zhavoronkov2019} or the combination of the compound with other drugs
\cite{Zitnik2018}---a common practice in patients with cancer. All these possible
changes on the critic model would apply at different abstraction levels. That opens the
door to seek for ways to integrate the representations learnt at those different stages
\cite{Ying2018, Ma2019, Huang2019}. On top of that information extracted from here could
be then leveraged on the drug generation part of the framework.


    \section*{Data \& Baselines}

The data from this project will be obtained from multiple sources including, but not
limited to, the Genomics of Drug Sensitivity in Cancer (GDSC) databse \cite{Yang2013},
STRING and STICH, \cite{Szklarczyk2019,Szklarczyk2016}, DrugBank \cite{Wishart2006},
ChEMBL \cite{Gaulton2017}, PubChem \cite{Kim2019} and ZINC 15 \cite{Sterling2015}. For
the development of the models PyTorch \cite{Paszke2019} will be used as a main
framework, likely used alongside DeepChem\footnote{https://deepchem.io/}.

The novel predictive model will be evaluated against empirically measured properties of
drugs in the mentioned data sources. Furthermore model's performance will be compared to
that of other models in the literature. For instance, comparing IC50 prediction values
for for cell-drug paris (unseen during training) to the empirically measured ones from
the previously mentioned data sources as done in \cite{Oskooei2019, Joo2019,
Oskooei2018}. Another prospective set of benchmarks to be used are those proposed by
MoleculeNet \cite{Wu2018}.

% To evaluate a generative model one should compute how well it estimates the density
% distribution over the space of possible compounds. Unfortunately such thing is usually
% impossible \cite{Theis2016} 
For evaluation of generative models previous work \cite{Theis2016} has discussed how
different metrics measure different conceptions of performance. In order to deal with
different considerations, such as diversity, novelty, molecular activity, or stability,
metrics like the Fréchet ChemNet Distance (FCD) \cite{Preuer} have been proposed. In
this project, one the metrics will come from the predictive model, which will be used
method to evaluate the effectiveness of the generated compounds (i.e. the performance of
the generative model). At the same time, another metric for the generative model will be
the similarity of the generated compounds to already existing drugs. An approach that
has already been used in the literature \cite{Born2019}. A wider benchmarking framework,
GucaMol, was proposed recently \cite{Brown2019} and could be used in this project. On
top of all these metrics, a natural final step for the entire evaluation of the system
would be the actual in-vitro synthesis of the generated compounds, as done in
\cite{Zhavoronkov2019}, however the complexity of that step makes it unlikely to be
carried out during the execution of this project. 

% \subsection*{Practicalities}

% Work at IBM?

% \subsection*{Something else?}

% \section*{Software and Hardware Requirements}
% Outline what your specific requirements will be with regard
% to software and hardware, but note that any special requests
% might need to be approved by your supervisor and the Head of
% Department.

% Overall, you should aim to produce roughly a two-page document
% (and certainly no more than four pages)
% outlining your plan for the year.


\bibliographystyle{apalike}
\bibliography{thesis-proposal.bib}
    
\end{document}    
